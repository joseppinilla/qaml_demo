{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classical RBM training on the Bars-And-Stripes Dataset for Reconstruction\n",
        "This is an example on classical Gibbs training of an RBM on the BAS(4,4)\n",
        "dataset.\n",
        "Developed by: Jose Pinilla"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required packages\n",
        "import qaml\n",
        "import torch\n",
        "torch.manual_seed(0) # For deterministic weights\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as torch_transforms\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################# Hyperparameters ##############################\n",
        "M,N = SHAPE = (8,8)\n",
        "DATA_SIZE = N*M\n",
        "HIDDEN_SIZE = 64\n",
        "EPOCHS = 200\n",
        "SAMPLES = None\n",
        "BATCH_SIZE = 400\n",
        "TRAIN,TEST = SPLIT = 400,110\n",
        "# Stochastic Gradient Descent\n",
        "learning_rate = 0.1\n",
        "weight_decay = 1e-4\n",
        "momentum = 0.5\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################### Input Data ################################\n",
        "bas_dataset = qaml.datasets.BAS(*SHAPE,embed_label=True,transform=torch.Tensor)\n",
        "train_dataset,test_dataset = torch.utils.data.random_split(bas_dataset,[*SPLIT])\n",
        "train_sampler = torch.utils.data.RandomSampler(train_dataset,replacement=False,\n",
        "                                               num_samples=SAMPLES)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,sampler=train_sampler,\n",
        "                                           batch_size=BATCH_SIZE)\n",
        "\n",
        "# PLot all data\n",
        "fig,axs = plt.subplots(6,5)\n",
        "for ax,(img,label) in zip(axs.flat,train_dataset):\n",
        "    ax.matshow(img.view(*SHAPE),vmin=0,vmax=1); ax.axis('off')\n",
        "plt.tight_layout()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################# Model Definition #############################\n",
        "# Specify model with dimensions\n",
        "rbm = qaml.nn.RBM(DATA_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "# Initialize biases\n",
        "torch.nn.init.constant_(rbm.b,0.5)\n",
        "torch.nn.init.zeros_(rbm.c)\n",
        "torch.nn.init.uniform_(rbm.W,-0.5,0.5)\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = torch.optim.SGD(rbm.parameters(), lr=learning_rate,\n",
        "                            weight_decay=weight_decay,momentum=momentum)\n",
        "\n",
        "# Set up training mechanisms\n",
        "gibbs_sampler = qaml.sampler.GibbsNetworkSampler(rbm)\n",
        "CD = qaml.autograd.SampleBasedConstrastiveDivergence()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################## Model Training ##############################\n",
        "# Set the model to training mode\n",
        "rbm.train()\n",
        "err_log = []\n",
        "accuracy_log = []\n",
        "b_log = [rbm.b.detach().clone().numpy()]\n",
        "c_log = [rbm.c.detach().clone().numpy()]\n",
        "W_log = [rbm.W.detach().clone().numpy().flatten()]\n",
        "for t in range(EPOCHS):\n",
        "    epoch_error = torch.Tensor([0.])\n",
        "    for img_batch, labels_batch in train_loader:\n",
        "        input_data = img_batch.flatten(1)\n",
        "\n",
        "        # Positive Phase\n",
        "        v0, prob_h0 = input_data, rbm(input_data)\n",
        "        # Negative Phase\n",
        "        vk, prob_hk = gibbs_sampler(v0.detach(), k=5)\n",
        "\n",
        "        # Reconstruction error from Contrastive Divergence\n",
        "        err = CD.apply((v0,prob_h0), (vk,prob_hk), *rbm.parameters())\n",
        "\n",
        "        # Do not accumulate gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute gradients\n",
        "        err.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        #Accumulate error for this epoch\n",
        "        epoch_error  += err\n",
        "\n",
        "    # Error Log\n",
        "    b_log.append(rbm.b.detach().clone().numpy())\n",
        "    c_log.append(rbm.c.detach().clone().numpy())\n",
        "    W_log.append(rbm.W.detach().clone().numpy().flatten())\n",
        "    err_log.append(epoch_error.item())\n",
        "    print(f\"Epoch {t} Reconstruction Error = {epoch_error.item()}\")\n",
        "    ############################## CLASSIFICATION ##################################\n",
        "    count = 0\n",
        "    for test_data, test_label in test_dataset:\n",
        "        test_data[-2:,-1] = 0.5\n",
        "        prob_hk = rbm(test_data.flatten())\n",
        "        label_pred = rbm.generate(prob_hk).view(*SHAPE)[-2:,-1]\n",
        "        if label_pred.argmax() == test_label.argmax():\n",
        "            count+=1\n",
        "    accuracy_log.append(count/TEST)\n",
        "    print(f\"Testing accuracy: {count}/{TEST} ({count/TEST:.2f})\")\n",
        "# Set the model to evaluation mode\n",
        "# rbm.eval()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################## Logging Directory ###############################\n",
        "import os\n",
        "directory = 'BAS88_classical5_200_Adapt'\n",
        "if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "seed = torch.initial_seed()\n",
        "if not os.path.exists(f'{directory}/{seed}'):\n",
        "        os.makedirs(f'{directory}/{seed}')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################ Store Model and Logs ##############################\n",
        "torch.save(b_log,f\"./{directory}/{seed}/b.pt\")\n",
        "torch.save(c_log,f\"./{directory}/{seed}/c.pt\")\n",
        "torch.save(W_log,f\"./{directory}/{seed}/W.pt\")\n",
        "torch.save(err_log,f\"./{directory}/{seed}/err.pt\")\n",
        "torch.save(accuracy_log,f\"./{directory}/{seed}/accuracy.pt\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################## Sampling ####################################\n",
        "num_samples = 1000\n",
        "prob_v,_ = gibbs_sampler(torch.rand(num_samples,DATA_SIZE),k=10)\n",
        "img_samples = prob_v.view(num_samples,*SHAPE).bernoulli()\n",
        "# PLot some samples\n",
        "fig,axs = plt.subplots(4,5)\n",
        "for ax,img in zip(axs.flat,img_samples):\n",
        "    ax.matshow(img.view(*SHAPE),vmin=0,vmax=1); ax.axis('off')\n",
        "plt.tight_layout()\n",
        "# Get and print score\n",
        "p,r,score = bas_dataset.score(img_samples)\n",
        "print(f\"qBAS : Precision = {p:.02} Recall = {r:.02} Score = {score:.02}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################## RECONSTRUCTION ##################################\n",
        "k = 10\n",
        "hist = {}\n",
        "count = 0\n",
        "mask = torch_transforms.functional.erase(torch.ones(1,M,N),2,2,4,4,0).flatten()\n",
        "for img, label in train_dataset:\n",
        "    clamped = mask*(img.flatten().detach().clone())\n",
        "    prob_hk = rbm.forward(clamped + (1-mask)*0.5)\n",
        "    prob_vk = rbm.generate(prob_hk).detach()\n",
        "    for _ in range(k):\n",
        "        masked = clamped + (1-mask)*prob_vk.data\n",
        "        prob_hk.data = rbm.forward(masked).data\n",
        "        prob_vk.data = rbm.generate(prob_hk).data\n",
        "    recon = (clamped + (1-mask)*prob_vk).bernoulli().view(img.shape)\n",
        "    if recon.equal(img):\n",
        "        count+=1\n",
        "    num = torch.count_nonzero(recon.to(bool).bitwise_xor(img.to(bool))).item()\n",
        "    hist[num]=hist.get(num,0)+1\n",
        "print(f\"Dataset Reconstruction: {count/(TEST+TRAIN):.02}\")\n",
        "plt.bar(hist.keys(),hist.values())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################ MODEL VISUALIZATION ###############################\n",
        "# Testing accuracy graph\n",
        "fig, ax = plt.subplots()\n",
        "plt.plot(accuracy_log)\n",
        "plt.ylabel(\"Testing Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.savefig(\"classical_accuracy.pdf\")\n",
        "\n",
        "# L1 error graph\n",
        "fig, ax = plt.subplots()\n",
        "plt.plot(err_log)\n",
        "plt.ylabel(\"Reconstruction Error (L1)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.savefig(\"classical_err_log.pdf\")\n",
        "\n",
        "# Visible bias graph\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle('color', list(plt.get_cmap('turbo',DATA_SIZE).colors))\n",
        "lc_v = ax.plot(b_log)\n",
        "plt.legend(lc_v,[f'b{i}' for i in range(DATA_SIZE)],ncol=4,loc=(0,1))\n",
        "plt.ylabel(\"Visible Biases\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.savefig(\"classival_b_log.pdf\")\n",
        "\n",
        "# Hidden bias graph\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle('color', list(plt.get_cmap('turbo',HIDDEN_SIZE).colors))\n",
        "lc_h = plt.plot(c_log)\n",
        "plt.legend(lc_h,[f'c{i}' for i in range(HIDDEN_SIZE)],ncol=4,loc=(0,1))\n",
        "plt.ylabel(\"Hidden Biases\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.savefig(\"classical_c_log.pdf\")\n",
        "\n",
        "# Weights graph\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle('color', list(plt.get_cmap('turbo',rbm.V*rbm.H).colors))\n",
        "lc_w = plt.plot(W_log)\n",
        "plt.ylabel(\"Weights\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.savefig(\"classical_W_log.pdf\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################## ENERGY ######################################\n",
        "data_energies = []\n",
        "for img,label in bas_dataset:\n",
        "    data = img.flatten()\n",
        "    data_energies.append(rbm.free_energy(data).item())\n",
        "\n",
        "rand_energies = []\n",
        "rand_data = torch.rand(len(bas_dataset)*10,rbm.V)\n",
        "for img in rand_data:\n",
        "    rand_energies.append(rbm.free_energy(img.bernoulli()).item())\n",
        "\n",
        "gibbs_energies = []\n",
        "for img,label in bas_dataset:\n",
        "    data = img.flatten()\n",
        "    prob_v,prob_h = gibbs_sampler(data,k=5)\n",
        "    gibbs_energies.append(rbm.free_energy(prob_v.bernoulli()).item())\n",
        "\n",
        "qa_energies = []\n",
        "solver_name = \"Advantage_system1.1\"\n",
        "qa_sampler = qaml.sampler.QASampler(rbm,solver=solver_name)\n",
        "qa_sampleset = qa_sampler(num_reads=BATCH_SIZE,auto_scale=False)\n",
        "\n",
        "for s_v,s_h in zip(*qa_sampleset):\n",
        "    qa_energies.append(rbm.free_energy(s_v.detach()).item())\n",
        "\n",
        "plot_data = [(data_energies,  'Data',    'blue'),\n",
        "             (rand_energies,  'Random',  'red'),\n",
        "             (gibbs_energies, 'Gibbs-5', 'green'),\n",
        "             (qa_energies,    'Quantum', 'orange')]\n",
        "\n",
        "hist_kwargs = {'ec':'k','lw':2.0,'alpha':0.5,'histtype':'stepfilled','bins':100}\n",
        "weights = lambda data: [1./len(data) for _ in data]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "for data,name,color in plot_data:\n",
        "    ax.hist(data,weights=weights(data),label=name,color=color,**hist_kwargs)\n",
        "\n",
        "plt.xlabel(\"Energy\")\n",
        "plt.ylabel(\"Count/Total\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig(\"classical_energies.pdf\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################## VISUALIZE ###################################\n",
        "plt.matshow(rbm.b.detach().view(*SHAPE))\n",
        "plt.colorbar()\n",
        "plt.savefig(\"classical_b.pdf\")\n",
        "plt.matshow(rbm.c.detach().view(1,HIDDEN_SIZE))\n",
        "plt.yticks([])\n",
        "plt.colorbar()\n",
        "plt.savefig(\"classical_c.pdf\")\n",
        "\n",
        "fig,axs = plt.subplots(HIDDEN_SIZE//4,4)\n",
        "for i,ax in enumerate(axs.flat):\n",
        "    weight_matrix = rbm.W[i].detach().view(*SHAPE)\n",
        "    ms = ax.matshow(weight_matrix)\n",
        "    ax.axis('off')\n",
        "fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "cbar = fig.colorbar(ms, ax=axs.ravel().tolist(), shrink=0.95)\n",
        "plt.savefig(\"classical_weights.pdf\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}