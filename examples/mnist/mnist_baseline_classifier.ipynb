{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This example does not use QAML\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.datasets as torch_datasets\n",
        "import torchvision.transforms as torch_transforms\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension\n",
        "N, D_in, H, D_out = 64, 784, 128, 10\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################# Hyperparameters ##############################\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "# Stochastic Gradient Descent\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1e-4\n",
        "momentum = 0.5\n",
        "# Contrastive Divergence (CD-k)\n",
        "cd_k = 1\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################### Input Data ################################\n",
        "train_dataset = torch_datasets.MNIST(root='./data/', train=True,\n",
        "                                     transform=torch_transforms.ToTensor(),\n",
        "                                     download=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_dataset = torch_datasets.MNIST(root='./data/', train=False,\n",
        "                                    transform=torch_transforms.ToTensor(),\n",
        "                                    download=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################## Model Training ##############################\n",
        "model = torch.nn.Sequential(torch.nn.Linear(D_in, H),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.Linear(H, D_out),)\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "for t in range(EPOCHS):\n",
        "    for v_batch, labels_batch in train_loader:\n",
        "        # Forward pass: compute predicted y by passing x to the model.\n",
        "        y_pred = model(v_batch.view(len(v_batch),D_in))\n",
        "\n",
        "        # Compute and print loss.\n",
        "        loss = loss_fn(y_pred,  torch.nn.functional.one_hot(labels_batch,10)*1.0)\n",
        "\n",
        "        # Before the backward pass, use the optimizer object to zero all of the\n",
        "        # gradients for the Tensors it will update (which are the learnable weights\n",
        "        # of the model)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Calling the step function on an Optimizer makes an update to its parameters\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {t} Loss = {loss.item()}\")\n",
        "\n",
        "count = 0\n",
        "for test_data, test_label in test_loader:\n",
        "    label_pred = model(test_data.view(1,D_in)).argmax()\n",
        "    if label_pred == test_label:\n",
        "        count+=1\n",
        "print(f\"Testing accuracy: {count}/{len(test_dataset)}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}