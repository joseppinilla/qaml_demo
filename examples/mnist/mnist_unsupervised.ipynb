{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "RBM unsupervised training example of MNIST using Persistent Contrastive\n",
        "Divergence (PCD-1).\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import qaml\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.datasets as torch_datasets\n",
        "import torchvision.transforms as torch_transforms\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################# Hyperparameters ##############################\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 64\n",
        "# Stochastic Gradient Descent\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1e-4\n",
        "momentum = 0.5\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################### Input Data ################################\n",
        "train_dataset = torch_datasets.MNIST(root='./data/', train=True,\n",
        "                                     transform=torch_transforms.ToTensor(),\n",
        "                                     download=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_dataset = torch_datasets.MNIST(root='./data/', train=False,\n",
        "                                    transform=torch_transforms.ToTensor(),\n",
        "                                    download=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################# Model Definition #############################\n",
        "DATA_SIZE = len(train_dataset.data[0].flatten())\n",
        "HIDDEN_SIZE = 128\n",
        "\n",
        "# Specify model with dimensions\n",
        "rbm = qaml.nn.RBM(DATA_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = torch.optim.SGD(rbm.parameters(), lr=learning_rate,\n",
        "                                              weight_decay=weight_decay,\n",
        "                                              momentum=momentum)\n",
        "# Set up training mechanisms\n",
        "sampler = qaml.sampler.PersistentGibbsNetworkSampler(rbm, BATCH_SIZE)\n",
        "CD = qaml.autograd.ConstrastiveDivergence()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################## Model Training ##############################\n",
        "# Set the model to training mode\n",
        "rbm.train()\n",
        "err_log = []\n",
        "for t in range(EPOCHS):\n",
        "    epoch_error = torch.Tensor([0.])\n",
        "    for img_batch, labels_batch in train_loader:\n",
        "\n",
        "        input_data = img_batch.flatten(1)\n",
        "\n",
        "        # Positive Phase\n",
        "        v0, prob_h0 = input_data, rbm(input_data)\n",
        "        # Negative Phase\n",
        "        vk, prob_hk = sampler(len(v0), k=1)\n",
        "\n",
        "        # Reconstruction error from Contrastive Divergence\n",
        "        err = CD.apply((v0,prob_h0), (vk,prob_hk), *rbm.parameters())\n",
        "\n",
        "        # Do not accumulated gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Compute gradients. Save compute graph at last epoch\n",
        "        err.backward(retain_graph=(t == EPOCHS-1))\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        epoch_error  += err\n",
        "    err_log.append(epoch_error.item())\n",
        "    print(f\"Epoch {t} Reconstruction Error = {epoch_error.item()}\")\n",
        "\n",
        "plt.plot(err_log)\n",
        "plt.ylabel(\"Reconstruction Error\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "rbm.eval()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################# VISUALIZE ####################################\n",
        "# Computation Graph\n",
        "from torchviz import make_dot\n",
        "make_dot(err)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option to save for future use\n",
        "torch.save(rbm,\"mnist_unsupervised.pt\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option to load existing model\n",
        "rbm = torch.load(\"mnist_unsupervised.pt\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the distribution of energies for (a) the training data, (b) the test data\n",
        "(c) a set of random samples of visible configurations. The expected result is\n",
        "to have both (a) and (b) as clusters of lower energy, and (c) as a normal\n",
        "distribution to the right, i.e. high energy.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################## ENERGY ######################################\n",
        "data_energies = []\n",
        "for img,_ in train_dataset:\n",
        "    data_energies.append(rbm.free_energy(img.float().view(rbm.V)).item())\n",
        "\n",
        "test_energies = []\n",
        "for img,_ in test_dataset:\n",
        "    test_energies.append(rbm.free_energy(img.float().view(rbm.V)).item())\n",
        "\n",
        "rand_energies = []\n",
        "for _ in range(len(train_dataset)):\n",
        "    rand_energies.append(rbm.free_energy(torch.rand(rbm.V)).item())\n",
        "\n",
        "plt.hist(data_energies,label=\"Data\",bins=100)\n",
        "plt.hist(test_energies,label=\"Test\",bins=100)\n",
        "plt.hist(rand_energies,label=\"Random\",bins=100)\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(\"Energy\")\n",
        "plt.legend()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's sometimes useful to visualize the distribution of linear biases and\n",
        "weights between visible and hidden layers\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################# VISIBLE ######################################\n",
        "plt.matshow(rbm.b.detach().view(28, 28))\n",
        "plt.colorbar()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################# WEIGHTS ######################################\n",
        "fig,axs = plt.subplots(HIDDEN_SIZE//8,8)\n",
        "for i,ax in enumerate(axs.flat):\n",
        "    weight_matrix = rbm.W[i].detach().view(28, 28)\n",
        "    ms = ax.matshow(weight_matrix, cmap='viridis', vmin=-1, vmax=1)\n",
        "    ax.axis('off')\n",
        "fig.subplots_adjust(wspace=0.0, hspace=0.0)\n",
        "cbar = fig.colorbar(ms, ax=axs.ravel().tolist(), shrink=0.95)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's possible to sample from the joint probability of the model and plot the\n",
        "visible units of those samples. This doesn't necessarily sample an image of a\n",
        "number.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################## SAMPLE ######################################\n",
        "SAMPLES = 4\n",
        "prob_vk,prob_hk = sampler(SAMPLES,k=3,init=torch.rand(BATCH_SIZE,rbm.V)*0.1)\n",
        "fig,axs = plt.subplots(1,SAMPLES)\n",
        "for ax,vk in zip(axs.flat,prob_vk):\n",
        "    ax.matshow(vk.detach().view(28, 28))\n",
        "    ax.axis('off')\n",
        "fig.subplots_adjust(wspace=0.0, hspace=0.0)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################ NOISE RECONSTRUCTION ##############################\n",
        "input_data, label = train_loader.dataset[85] # Random input\n",
        "corrupt_data = (input_data + torch.randn_like(input_data)*0.5).view(1,784)\n",
        "prob_vk,prob_hk = sampler(1,k=1,init=corrupt_data.clone())\n",
        "recon_data = prob_vk.detach()\n",
        "\n",
        "fig,axs = plt.subplots(1,3)\n",
        "axs[0].matshow(input_data.view(28,28))\n",
        "axs[1].matshow(corrupt_data.view(28,28))\n",
        "axs[2].matshow(recon_data.detach().view(28,28))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################## RECONSTRUCTION ##################################\n",
        "input_data, label = train_loader.dataset[4]\n",
        "mask = torch.ones_like(input_data)\n",
        "for i in range(0,15): # Is there a nicer way to create random masks?\n",
        "    for j in range(0,15):\n",
        "        mask[0][j][i] = 0\n",
        "\n",
        "corrupt_data = (input_data*mask).view(1,784)\n",
        "\n",
        "prob_vk,prob_hk = sampler(1,k=1,init=corrupt_data.clone())\n",
        "fig,axs = plt.subplots(1,3)\n",
        "axs[0].matshow(input_data.view(28, 28))\n",
        "axs[1].matshow(corrupt_data.view(28, 28))\n",
        "axs[2].matshow(prob_vk.detach().view(28, 28))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though this model was trained unsupervised, it is possible to now use it\n",
        "as a pre-trained model and \"fit\" its hidden layers and output and perform\n",
        "training on that.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################### CLASSIFIER ####################################\n",
        "LABEL_SIZE = len(train_dataset.classes)\n",
        "\n",
        "model = torch.nn.Sequential(rbm,\n",
        "                            torch.nn.Linear(HIDDEN_SIZE,LABEL_SIZE),)\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "for t in range(10):\n",
        "    for v_batch, labels_batch in train_loader:\n",
        "        # Forward pass: compute predicted y by passing x to the model.\n",
        "        y_pred = model(v_batch.view(len(v_batch),DATA_SIZE))\n",
        "\n",
        "        # Compute and print loss.\n",
        "        loss = loss_fn(y_pred, torch.nn.functional.one_hot(labels_batch,10)*1.0)\n",
        "\n",
        "        # Before the backward pass, use the optimizer object to zero all of the\n",
        "        # gradients for the Tensors it will update (which are the learnable weights\n",
        "        # of the model)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Calling the step function on an Optimizer makes an update to its parameters\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {t} Loss = {loss.item()}\")\n",
        "\n",
        "count = 0\n",
        "for test_data, test_label in test_loader:\n",
        "    label_pred = model(test_data.view(1,DATA_SIZE)).argmax()\n",
        "    if label_pred == test_label:\n",
        "        count+=1\n",
        "print(f\"Testing accuracy: {count}/{len(test_dataset)}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}